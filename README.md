# Proyecto ETL - Productos de FakeStore API

Un pipeline ETL (Extract, Transform, Load) desarrollado en Python que extrae datos de productos desde la FakeStore API, los transforma usando Apache Spark y los carga en una base de datos PostgreSQL.

## ğŸ“‹ DescripciÃ³n

Este proyecto implementa un pipeline ETL completo que:

1. **Extrae** datos de productos desde la [FakeStore API](https://fakestoreapi.com/products)
2. **Transforma** los datos usando Apache Spark para limpiarlos y estructurarlos
3. **Carga** los datos procesados en una base de datos PostgreSQL
4. **Genera** un reporte de productos por categorÃ­a

## ğŸ—ï¸ Arquitectura del Proyecto

Se opta por un enfoque modular para una mejor organizaciÃ³n de los elementos del proyecto tanto para las 
etapas de ETL como de las configuraciones de bases de datos, sesiÃ³n en spark y el manejo de excepciones.

```
ETL/
â”œâ”€â”€ main.py                 # Archivo principal del pipeline ETL
â”œâ”€â”€ requirements.txt        # Dependencias de Python
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ config.ini         # ConfiguraciÃ³n de la base de datos
â”‚   â””â”€â”€ postgresql-42.7.8.jar  # Driver JDBC de PostgreSQL
â”œâ”€â”€ core/
â”‚   â””â”€â”€ exceptions.py      # Excepciones personalizadas
â”œâ”€â”€ etl/
â”‚   â”œâ”€â”€ extract.py         # MÃ³dulo de extracciÃ³n de datos
â”‚   â”œâ”€â”€ transform.py       # MÃ³dulo de transformaciÃ³n con Spark
â”‚   â””â”€â”€ load.py           # MÃ³dulo de carga a PostgreSQL
â””â”€â”€ utils/
    â””â”€â”€ spark_session.py   # ConfiguraciÃ³n de sesiÃ³n de Spark
```

## ğŸ› ï¸ TecnologÃ­as Utilizadas

- **Python 3.11+**
- **Apache Spark (PySpark)** - Para procesamiento de datos
- **PostgreSQL** - Base de datos de destino
- **Requests** - Para consumo de API REST
- **ConfigParser** - Para gestiÃ³n de configuraciÃ³n

## ğŸ“¦ Requisitos

### Dependencias de Python
```bash
pyspark
requests
configparser
psycopg2-binary
```

### Requisitos del Sistema
- Python 3.11 o superior
- Java 8+ (requerido por Spark)
- PostgreSQL
- Acceso a internet (para consumir la API)

## ğŸš€ InstalaciÃ³n y ConfiguraciÃ³n

### 1. Clonar el repositorio
```bash
git clone <tu-repositorio>
cd Projects
```

### 2. Crear entorno virtual
```bash
python -m venv env
source env/bin/activate  # En macOS/Linux
# o
env\Scripts\activate     # En Windows
```

### 3. Instalar dependencias
```bash
pip install -r ETL/requirements.txt
```

### 4. Configurar PostgreSQL

Crea una base de datos PostgreSQL y ajusta la configuraciÃ³n en `ETL/config/config.ini`:

```ini
[postgres]
user = user
password = password
host = localhost
port = 5432
database = tu_base_de_datos
jdbc_driver_path = config/postgresql-42.7.8.jar
```

### 5. Plantilla de configuraciÃ³n (opcional)
Si se planea mantener una configuraciÃ³n privada, puedes usar `config.ini.template` como base.

## â–¶ï¸ EjecuciÃ³n

Para ejecutar el pipeline ETL completo:

```bash
cd ETL
python main.py
```

## ğŸ“Š Funcionalidades

### ExtracciÃ³n
- Consume la FakeStore API para obtener datos de productos
- Manejo de errores de conexiÃ³n y respuesta
- ValidaciÃ³n de cÃ³digos de estado HTTP

### TransformaciÃ³n
- Limpieza y tipado de datos
- EstructuraciÃ³n de datos anidados (ratings)
- CreaciÃ³n de DataFrame de Spark con esquema definido
- GeneraciÃ³n de reporte agregado por categorÃ­a

### Carga
- InserciÃ³n de datos en PostgreSQL usando JDBC
- CreaciÃ³n de dos tablas:
  - `productos`: Datos principales de productos
  - `reporte_productos_por_categoria`: Reporte agregado

## ğŸ”§ Estructura de Datos

### Esquema de Productos
```python
- id (Long): Identificador Ãºnico del producto
- title (String): Nombre del producto
- price (Double): Precio del producto
- description (String): DescripciÃ³n del producto
- category (String): CategorÃ­a del producto
- image (String): URL de la imagen
- rating_rate (Double): CalificaciÃ³n promedio
- rating_count (Long): NÃºmero de calificaciones
```

## ğŸš¨ Manejo de Errores

El proyecto incluye excepciones personalizadas para el manejo de errores en cada etapa del ETL:

- `ExtractError`: Errores durante la extracciÃ³n de datos
- `TransformError`: Errores durante la transformaciÃ³n
- `LoadError`: Errores durante la carga de datos

## ğŸ“ˆ Monitoreo y Logs

- Logs detallados en cada etapa del proceso
- VisualizaciÃ³n de datos usando `show()` de Spark
- InformaciÃ³n de Ã©xito/error al finalizar el proceso

## ğŸ¤ Contribuciones

Las contribuciones son bienvenidas. Por favor:

1. Haz fork del proyecto
2. Crea una rama para tu feature (`git checkout -b feature/AmazingFeature`)
3. Commit tus cambios (`git commit -m 'Add some AmazingFeature'`)
4. Push a la rama (`git push origin feature/AmazingFeature`)
5. Abre un Pull Request


## ğŸ”„ Versiones

- **v1.0.0** - VersiÃ³n inicial con funcionalidad ETL completa
